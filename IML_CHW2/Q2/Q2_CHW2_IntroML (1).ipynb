{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChC3RF8meAlK"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**: **<font color='blue'>Matin Alinejad**\n",
    "\n",
    "**Student ID**: **<font color='green'>99101943**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IraiR0SbeDi_"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQjwWC3eDnc"
   },
   "source": [
    "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np # Replace with 'import cupy as np' to run on GPU\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.01, n_iter = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _loss(self, y, y_hat):\n",
    "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return np.where(y_predicted > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-i-oubUlZ6e"
   },
   "source": [
    "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KXzIy_2u-pG",
    "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "Precision: 0.875\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Logistic_question.csv')\n",
    "\n",
    "# Make target binary\n",
    "data['Target'] = (data['Target'] >= 0.5).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data['Target'], test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = MyLogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji0RXNGKv1pa"
   },
   "source": [
    "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldveD35twRRZ"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "The evaluation metrics used to assess the performance of the logistic regression model include accuracy, precision, recall, and the F1 score. Each of these metrics provides a different perspective on how well the model performs, especially in different contexts or for various tasks. Here's a breakdown of each metric and situations where they are particularly informative:\n",
    "\n",
    "### 1. **Accuracy**\n",
    "- **Definition**: Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. It is calculated as $$\\textbf{Accuracy} = \\mathbf{\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}}.$$\n",
    "\n",
    "\n",
    "- **Useful Contexts**: Accuracy is a useful measure when the classes in the dataset are well balanced. It gives a quick measure of overall correctness. However, its usefulness decreases when dealing with imbalanced datasets, as it can be misleadingly high when the majority class is predicted correctly while largely ignoring the minority class.\n",
    "\n",
    "### 2. **Precision**\n",
    "- **Definition**: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is calculated as $$\\textbf{Precision} = \\mathbf{\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}}.$$\n",
    "\n",
    "\n",
    "- **Useful Contexts**: Precision is crucial in scenarios where the cost of a false positive is high. For example, in email spam detection, a high precision model minimizes the risk of classifying important emails as spam. In medical testing, high precision minimizes false positive diagnoses, which can prevent unnecessary treatments that might be expensive or harmful.\n",
    "\n",
    "### 3. **Recall** (also known as Sensitivity or True Positive Rate)\n",
    "- **Definition**: Recall is the ratio of correctly predicted positive observations to all observations in actual class - yes. It is calculated as $$\\textbf{Recall} = \\mathbf{\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}}.$$\n",
    "\n",
    "\n",
    "- **Useful Contexts**: Recall is particularly important in cases where missing a positive instance is significantly worse than getting a false positive. For example, in fraud detection or disease screening, a high recall model ensures that most fraudulent activities or diseases are caught, even if some false alarms occur.\n",
    "\n",
    "### 4. **F1 Score**\n",
    "- **Definition**: The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is calculated as $$\\textbf{F1} = \\mathbf{2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}}.$$\n",
    "\n",
    "\n",
    "- **Useful Contexts**: The F1 score is useful when you want to strike a balance between Precision and Recall. It is particularly useful in scenarios where an uneven class distribution exists (i.e., when there are many more negatives than positives or vice versa). For example, in customer churn prediction or in any classification task where false positives and false negatives have a significant cost.\n",
    "\n",
    "Each of these metrics helps to provide a fuller picture of how a model is performing, and depending on the specific requirements and consequences of a task, some metrics may be more important than others. The choice of which metric to prioritize should align with the business or clinical objectives, and the nature of the consequences for incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZCeRHZSw-mh"
   },
   "source": [
    "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Vb5lRSQXDLR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9375\n",
      "Precision: 0.9333333333333333\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCvIymmMy_ji"
   },
   "source": [
    "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY0ohM16z3De"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "Based on the results you've shared, it's clear that the built-in `LogisticRegression` function from scikit-learn outperforms the custom implementation on most metrics. Here’s a comparison focusing on both performance and parameters:\n",
    "\n",
    "### Performance Comparison\n",
    "- **Accuracy**: The built-in model achieved an accuracy of 93.75% compared to 87.5% from the custom model. This suggests that the built-in model generally makes more correct predictions on the given dataset.\n",
    "- **Precision**: The precision of the built-in model is slightly higher at 93.33% versus 87.5% for the custom model, indicating that the built-in model has a lower rate of false positives.\n",
    "- **Recall**: Both models achieved a recall of 100%, which means both models were able to identify all the actual positives.\n",
    "- **F1 Score**: The F1 score, which balances precision and recall, is higher for the built-in model (96.55%) compared to the custom model (93.33%). This indicates a better overall balance of precision and recall in the built-in model.\n",
    "\n",
    "### Parameters and Their Impact\n",
    "The built-in `LogisticRegression` function in scikit-learn comes with several parameters that can significantly impact the model's performance:\n",
    "\n",
    "- **`solver`**: This parameter determines the algorithm to use for optimization. Common options include 'liblinear', 'newton-cg', 'lbfgs', 'sag', and 'saga'. Each solver works best under different conditions regarding data size, dimensionality, and whether the data fits well into memory.\n",
    "  \n",
    "- **`penalty`**: Specifies the norm used in the penalization. The most common are 'l2' (default) and 'l1'. Regularization is a technique used to reduce overfitting by discouraging overly complex models in some way. The choice of penalty can affect both the performance and convergence speed of the model.\n",
    "  \n",
    "- **`C`**: This is the inverse of regularization strength—smaller values specify stronger regularization. It can control the trade-off between achieving a low training error and a low testing error that is, the trade-off between underfitting and overfitting.\n",
    "\n",
    "- **`max_iter`**: Defines the maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "- **`class_weight`**: This parameter is used to adjust the weights inversely proportional to class frequencies in the input data. It’s especially useful for dealing with imbalanced datasets.\n",
    "\n",
    "### Conclusion\n",
    "The performance difference between the custom model and the built-in model could be attributed to several factors including the optimization algorithm, regularization, and convergence criteria used by the built-in model. Scikit-learn's `LogisticRegression` likely benefits from more sophisticated numerical optimization techniques as well as better handling of class imbalance and regularization which helps to prevent overfitting.\n",
    "\n",
    "In general, while a custom implementation can be educational and give you detailed control over the modeling process, built-in functions are typically more robust, efficient, and well-optimized for general use cases. For practical applications, especially on larger datasets or complex real-world scenarios, leveraging built-in libraries like scikit-learn is advisable due to their maturity and extensive testing across various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMqoYlr2kr7"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukvlqDe52xP5"
   },
   "source": [
    "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5Ir-_hFt286t"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np  # Replace with 'import cupy as np' to run on GPU\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MyMultinomialLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, n_classes=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _cross_entropy(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        return -np.sum(y * np.log(y_hat + 1e-8)) / m\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = np.max(y) + 1\n",
    "\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "\n",
    "        y_one_hot = np.eye(self.n_classes)[y]\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y_one_hot))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y_one_hot, axis=0)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        return np.argmax(y_predicted, axis=1)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your data and labels respectively:\n",
    "# model = MyMultinomialLogisticRegression(learning_rate=0.01, n_iter=1000)\n",
    "# model.fit(X_train, y_train)\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPQ3Rtay3Y2_"
   },
   "source": [
    "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9aP4QJPq29B3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.9375, 3: 0.8875, 4: 0.725, 5: 0.4875, 6: 0.475, 7: 0.375, 8: 0.4375, 9: 0.4125, 10: 0.225}\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "\n",
    "def quantize_target(data, levels):\n",
    "    \"\"\"This function quantizes the target data into 'levels' number of bins.\"\"\"\n",
    "    return pd.qcut(data, q = levels, labels = False, duplicates = 'drop')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Logistic_question.csv')\n",
    "features = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "results = {}\n",
    "for i in range(2, 11):  # Quantizing levels from 2 to 10\n",
    "    target = quantize_target(data['Target'], i)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = MyMultinomialLogisticRegression(learning_rate=0.01, n_iter = 1000, n_classes=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set and evaluate\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    results[i] = accuracy\n",
    "\n",
    "# Output results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2sHl5Z4dXi"
   },
   "source": [
    "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLERDAr4wnS"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "Based on the reported results for the multinomial logistic regression model with different quantization levels ($i$) of the target variable, we can observe that the model performs best when $i=2$. Here's a breakdown and analysis of the results:\n",
    "\n",
    "### Results Analysis\n",
    "- **Best Performance**: The highest accuracy is achieved when the target is divided into two levels ($i=2$), with an accuracy of 93.75%.\n",
    "- **Decreasing Accuracy with More Levels**: As the number of quantization levels increases, the accuracy generally decreases. This trend suggests that the model struggles with more complex classification tasks involving more classes.\n",
    "\n",
    "### Why Does Accuracy Decline?\n",
    "1. **Increased Complexity**: More levels mean more classes for the model to distinguish between. This can be more challenging, especially if the feature set does not contain enough discriminative information to separate more finely divided classes.\n",
    "2. **Data Imbalance**: Higher quantization may lead to imbalances in class distribution, which are notoriously difficult for models to handle effectively without specific interventions like balancing class weights or using different sampling techniques.\n",
    "3. **Overfitting Risks**: With more classes, the model may also be at a higher risk of overfitting, particularly if there are not enough training samples per class to generalize well.\n",
    "\n",
    "### Visualizing the Results\n",
    "To better visualize how the model's performance changes with different levels of quantization, we can plot the accuracy against the number of levels ($i$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "levels = list(range(2, 11))\n",
    "accuracies = [0.9375, 0.8875, 0.725, 0.4875, 0.475, 0.375, 0.4375, 0.4125, 0.225]\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(levels, accuracies, marker = 'o', linestyle = '-', color = 'b')\n",
    "plt.title('Model Performance vs. Number of Quantization Levels')\n",
    "plt.xlabel('Number of Levels ($i$)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(levels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization would clearly show the trend of decreasing performance with an increasing number of quantization levels.\n",
    "\n",
    "### Conclusion\n",
    "The best performance at $i=2$ could indicate that the underlying problem suits a binary classification better, or that the dataset contains features that are effectively discriminative only at a broader level. For higher values of $i$, enhancing model performance might require additional features, more complex models, or techniques specifically aimed at handling multi-class classification challenges such as adjusting class weights, increasing sample size, or employing more advanced regularization techniques.\n",
    "\n",
    "Given these results and insights, it might be beneficial to reevaluate the necessity of higher quantization levels or explore methods to better handle the increased complexity and potential data imbalances that come with them."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
